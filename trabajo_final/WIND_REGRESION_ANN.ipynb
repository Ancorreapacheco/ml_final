{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Ignorar advertencias ===\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# === Manejo del sistema y utilidades ===\n",
    "import os\n",
    "import ast\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange  # Aseg√∫rate de tenerlo importado\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "# === Manipulaci√≥n de datos ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# === Visualizaci√≥n ===\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Preprocesamiento y m√©tricas ===\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    mean_absolute_percentage_error,\n",
    "    r2_score\n",
    ")\n",
    "\n",
    "# === Diagn√≥stico estad√≠stico ===\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "# === Redes neuronales (Keras / TensorFlow) ===\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listar dispositivos disponibles\n",
    "devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if devices:\n",
    "    print(f\"TensorFlow est√° utilizando la GPU: {devices}\")\n",
    "else:\n",
    "    print(\"TensorFlow no est√° utilizando la GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresi√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/data_treino_dv_df_2000_2010.csv')\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['HORA','WIND_DIR_HOR','WIND_VEL_HOR','HUM_REL_MAX_ANT','HUM_REL_MIN_ANT','TEMP_MAX_ANT','TEMP_MIN_ANT','HUM_REL_HOR','PRES_ATM_NIV','PREC_HOR','RAFAGA_VIENTO','PRES_ATM_MAX_ANT','PRES_ATM_MIN_ANT']\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='HORA', inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos con escalamiento de variables\n",
    "\n",
    "Los siguientes modelos requieren escalamiento de las variables predictoras, por lo que se trabajan en la misma funci√≥n\n",
    "\n",
    "* MLP\n",
    "* RNN\n",
    "* LSTM\n",
    "\n",
    "En un libro aparte, se realiz√≥ una exploraci√≥n manual de hiperpar√°metros para cada uno de los modelos, y se estableci√≥ de forma experimental que los siguientes hiperpar√°metros son los que mejor se ajustan a cada uno de los modelos. En este libro, solo se modificar√° en el GridSearch uno o m√°ximo dos (2) par√°metros adicionales, para verificar si el modelo se ajusta mejor a los datos. \n",
    "\n",
    "* MLP: 2 capas ocultas laprimera de 64 neurnonas y la segunda de 32, activaci√≥n tanh, optimizador Adam. Se probaran valores de learning rate y regularizaci√≥n L2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Construye un modelo MLP con capas ocultas personalizadas\n",
    "def build_mlp_model(input_dim, hidden_layers, activation='relu', learning_rate=0.001, kernel_regularizer=None):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_layers[0], input_dim=input_dim, activation=activation, kernel_regularizer=kernel_regularizer))\n",
    "    for units in hidden_layers[1:]:\n",
    "        model.add(Dense(units, activation=activation, kernel_regularizer=kernel_regularizer))\n",
    "    model.add(Dense(1))  # Capa de salida (regresi√≥n)\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "    return model\n",
    "\n",
    "# üß† Construye un modelo RNN simple\n",
    "def build_rnn_model(timesteps, input_dim, hidden_units, activation='tanh', learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(hidden_units, activation=activation, input_shape=(timesteps, input_dim)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "    return model\n",
    "\n",
    "# üß† Construye un modelo LSTM para dependencias temporales largas\n",
    "def build_lstm_model(timesteps, input_dim, hidden_units, activation='tanh', learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_units, activation=activation, input_shape=(timesteps, input_dim)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "    return model\n",
    "\n",
    "# üß† Genera secuencias de entrenamiento para RNN y LSTM\n",
    "def create_rnn_sequences(df, target_col, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - window_size):\n",
    "        seq = df.iloc[i:i+window_size]\n",
    "        X.append(seq.drop(columns=[target_col]).values)\n",
    "        y.append(df.iloc[i+window_size][target_col])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# üîÅ Funci√≥n principal de sliding window para evaluaci√≥n de modelos\n",
    "def sliding_window_regression_models_scaling_keras_rnn(\n",
    "    df,\n",
    "    target_col='WIND_VEL_HOR',\n",
    "    T_values=[7, 14],\n",
    "    test_window=1,\n",
    "    model_types=['MLP'],\n",
    "    mlp_params={\n",
    "        'hidden_layers': [[64, 32]],\n",
    "        'activation': ['tanh'],\n",
    "        'learning_rate': [0.001, 0.01],\n",
    "        'epochs': [50],\n",
    "        'batch_size': [32],\n",
    "        'kernel_regularizer': [l2(0.001), l2(0.01)]\n",
    "    },\n",
    "    rnn_params={\n",
    "        'hidden_units': [16, 32],\n",
    "        'activation': ['tanh'],\n",
    "        'learning_rate': [0.001],\n",
    "        'epochs': [50],\n",
    "        'batch_size': [32]\n",
    "    },\n",
    "    lstm_params={\n",
    "        'hidden_units': [32, 64],\n",
    "        'activation': ['tanh'],\n",
    "        'learning_rate': [0.001],\n",
    "        'epochs': [50],\n",
    "        'batch_size': [32]\n",
    "    },\n",
    "    save_path='./progreso_keras_rnn'\n",
    "):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    resultados_por_T = {}\n",
    "\n",
    "    log_tiempos_path = os.path.join(save_path, 'tiempos_ejecucion.csv')\n",
    "    if not os.path.exists(log_tiempos_path):\n",
    "        with open(log_tiempos_path, 'w') as f:\n",
    "            f.write('T_dias,modelo,parametros,duracion_segundos\\n')\n",
    "\n",
    "    for T in tqdm(T_values, desc=\"Procesando ventanas T\"):\n",
    "        print(f\"\\nüß≠ Iniciando evaluaci√≥n para ventana T={T} d√≠as...\")\n",
    "        inicio_ventana = time.time()\n",
    "        T_hours = T * 24\n",
    "        test_hours = test_window * 24\n",
    "        total_windows = len(df) - T_hours - test_hours + 1\n",
    "        output_path = os.path.join(save_path, f'ANN_resultados_T{T}.csv')\n",
    "\n",
    "        # Cargar resultados previos si existen\n",
    "        if os.path.exists(output_path):\n",
    "            df_prev = pd.read_csv(output_path)\n",
    "            if 'params' in df_prev:\n",
    "                df_prev['params'] = df_prev['params'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "        else:\n",
    "            df_prev = pd.DataFrame(columns=['modelo', 'params', 'T_dias', 'T_horas', 'MAPE', 'MAE', 'RMSE', 'MSE', 'R2', 'LjungBox_p'])\n",
    "\n",
    "        for model_type in model_types:\n",
    "            if model_type == 'MLP':\n",
    "                param_grid = list(product(*mlp_params.values()))\n",
    "                param_keys = list(mlp_params.keys())\n",
    "            elif model_type == 'RNN':\n",
    "                param_grid = list(product(*rnn_params.values()))\n",
    "                param_keys = list(rnn_params.keys())\n",
    "            elif model_type == 'LSTM':\n",
    "                param_grid = list(product(*lstm_params.values()))\n",
    "                param_keys = list(lstm_params.keys())\n",
    "\n",
    "            for combo in param_grid:\n",
    "                param_dict = dict(zip(param_keys, combo))\n",
    "                ya_realizado = not df_prev.empty and ((df_prev['modelo'] == model_type) & (df_prev['params'].apply(lambda p: p == param_dict))).any()\n",
    "                if ya_realizado:\n",
    "                    rmse = df_prev[((df_prev['modelo'] == model_type) & (df_prev['params'].apply(lambda p: p == param_dict)))]['RMSE'].values[0]\n",
    "                    print(f\"‚è© Saltando modelo ya evaluado: {model_type} con hiperpar√°metros: {param_dict} y RMSE {rmse}\")\n",
    "                    continue\n",
    "\n",
    "                modelo_idx = param_grid.index(combo) + 1\n",
    "                total_modelos = len(param_grid)\n",
    "                progreso_modelo = (modelo_idx / total_modelos) * 100\n",
    "                print(f\"\\nüîß Modelo {modelo_idx}/{total_modelos} ({progreso_modelo:.1f}%) - {model_type} con hiperpar√°metros: {param_dict}\")\n",
    "\n",
    "                inicio_modelo = time.time()\n",
    "                resultados = {k: [] for k in ['MAPE', 'MAE', 'RMSE', 'MSE', 'R2', 'LjungBox_p']}\n",
    "\n",
    "                for start in tqdm(range(0, total_windows, 24), desc=f\"     ‚Ü™ Subventanas ({model_type})\", leave=False):\n",
    "                    if model_type == 'MLP':\n",
    "                        train = df.iloc[start: start + T_hours]\n",
    "                        test = df.iloc[start + T_hours: start + T_hours + test_hours]\n",
    "\n",
    "                        X_train = train.drop(columns=[target_col])\n",
    "                        y_train = train[target_col]\n",
    "                        X_test = test.drop(columns=[target_col])\n",
    "                        y_test = test[target_col]\n",
    "\n",
    "                        scaler = StandardScaler()\n",
    "                        X_train = scaler.fit_transform(X_train)\n",
    "                        X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "                        model = build_mlp_model(\n",
    "                            input_dim=X_train.shape[1],\n",
    "                            hidden_layers=param_dict['hidden_layers'],\n",
    "                            activation=param_dict['activation'],\n",
    "                            learning_rate=param_dict['learning_rate'],\n",
    "                            kernel_regularizer=param_dict['kernel_regularizer']\n",
    "                        )\n",
    "\n",
    "                    else:\n",
    "                        # Para RNN o LSTM\n",
    "                        continue\n",
    "                        data_window = df.iloc[start: start + T_hours + test_hours].copy()\n",
    "                        train_data = data_window.iloc[:T_hours]\n",
    "                        test_data = data_window.iloc[T_hours:]\n",
    "\n",
    "                        scaler = StandardScaler()\n",
    "                        X_train_scaled = scaler.fit_transform(train_data.drop(columns=[target_col]))\n",
    "                        X_test_scaled = scaler.fit_transform(test_data.drop(columns=[target_col]))\n",
    "\n",
    "                        train_scaled = pd.DataFrame(X_train_scaled, columns=train_data.columns.drop(target_col))\n",
    "                        train_scaled[target_col] = train_data[target_col].values\n",
    "                        test_scaled = pd.DataFrame(X_test_scaled, columns=test_data.columns.drop(target_col))\n",
    "                        test_scaled[target_col] = test_data[target_col].values\n",
    "                        scaled = pd.concat([train_scaled, test_scaled])\n",
    "\n",
    "                        X, y = create_rnn_sequences(scaled, target_col, T_hours)\n",
    "                        X_train, y_train = X[:-1], y[:-1]\n",
    "                        X_test, y_test = X[-1:], y[-1:]\n",
    "\n",
    "                        if model_type == 'RNN':\n",
    "                            model = build_rnn_model(X_train.shape[1], X_train.shape[2], param_dict['hidden_units'], param_dict['activation'], param_dict['learning_rate'])\n",
    "                        else:\n",
    "                            model = build_lstm_model(X_train.shape[1], X_train.shape[2], param_dict['hidden_units'], param_dict['activation'], param_dict['learning_rate'])\n",
    "\n",
    "                    early_stop = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "                    model.fit(X_train, y_train, epochs=param_dict['epochs'], batch_size=param_dict['batch_size'], verbose=0, callbacks=[early_stop])\n",
    "                    y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "                    # C√°lculo de m√©tricas\n",
    "                    residuals = y_test.values - y_pred if hasattr(y_test, 'values') else y_test - y_pred\n",
    "                    resultados['MAPE'].append(mean_absolute_percentage_error(y_test, y_pred))\n",
    "                    resultados['MAE'].append(mean_absolute_error(y_test, y_pred))\n",
    "                    resultados['RMSE'].append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "                    resultados['MSE'].append(mean_squared_error(y_test, y_pred))\n",
    "                    resultados['R2'].append(r2_score(y_test, y_pred))\n",
    "                    ljung_p = acorr_ljungbox(residuals, lags=[1], return_df=True)['lb_pvalue'].iloc[0] if len(residuals) >= 2 else np.nan\n",
    "                    resultados['LjungBox_p'].append(ljung_p)\n",
    "\n",
    "                # Guardar resultados de este modelo\n",
    "                nuevo_row = pd.DataFrame([{\n",
    "                    'modelo': model_type,\n",
    "                    'params': param_dict,\n",
    "                    'T_dias': T,\n",
    "                    'T_horas': T_hours,\n",
    "                    'MAPE': np.mean(resultados['MAPE']),\n",
    "                    'MAE': np.mean(resultados['MAE']),\n",
    "                    'RMSE': np.mean(resultados['RMSE']),\n",
    "                    'MSE': np.mean(resultados['MSE']),\n",
    "                    'R2': np.mean(resultados['R2']),\n",
    "                    'LjungBox_p': np.nanmean(resultados['LjungBox_p'])\n",
    "                }])\n",
    "                df_prev = pd.concat([df_prev, nuevo_row], ignore_index=True)\n",
    "                df_prev.to_csv(output_path, index=False)\n",
    "\n",
    "                # Guardar duraci√≥n\n",
    "                fin_modelo = time.time()\n",
    "                duracion = fin_modelo - inicio_modelo\n",
    "                with open(log_tiempos_path, 'a') as f:\n",
    "                    f.write(f'{T},{model_type},\"{param_dict}\",{duracion:.2f}\\n')\n",
    "\n",
    "                print(f\"‚úÖ Finalizado {model_type} con RMSE promedio: {np.mean(resultados['RMSE']):.4f} en {duracion:.2f} segundos\")\n",
    "\n",
    "        resultados_por_T[T] = df_prev\n",
    "        duracion_ventana = time.time() - inicio_ventana\n",
    "        print(f\"üïí Tiempo total para T={T} d√≠as: {duracion_ventana:.2f} segundos\")\n",
    "\n",
    "    return resultados_por_T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_por_T = sliding_window_regression_models_scaling_keras_rnn(df, target_col='WIND_VEL_HOR')\n",
    "\n",
    "# Ver los DataFrames en memoria\n",
    "df_w_7 = resultados_por_T[7]    # Para T=7 d√≠as\n",
    "df_w_14 = resultados_por_T[14]    # Para T=7 d√≠as\n",
    "df_w_21 = resultados_por_T[21]    # Para T=7 d√≠as\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenar por RMSE de mayor a menor\n",
    "df_w_7_sorted = df_w_7.sort_values(by=\"RMSE\", ascending=True)\n",
    "\n",
    "# Etiquetas combinadas modelo + params\n",
    "etiquetas = df_w_7_sorted['modelo'].astype(str) + ' ' + df_w_7_sorted['params'].astype(str)\n",
    "\n",
    "# Crear la gr√°fica\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(etiquetas, df_w_7_sorted['RMSE'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE por Modelo - T=7 d√≠as (ordenado de mayor a menor)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNeighboors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
